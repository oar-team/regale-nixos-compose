apiVersion: v1
kind: Pod
metadata:
  labels:
    run: spark-app-pi
  name: spark-app-pi
  namespace: default
spec:
  # Required to be able to run pods
  serviceAccount: spark
    #securityContext:
    #  runAsUser: 1001
    #  runAsGroup: 100
  containers:
  - args:
    - driver
    - --master
    - k8s://https://kubernetes.default:443
    - --name
    - spark-pi
    - --conf
    - spark.executor.instances=8
    # Kube specific conf
    - --conf
    - spark.kubernetes.executor.request.cores=8
    - --conf
    - spark.kubernetes.executor.podTemplateFile=/etc/config/executor.yaml
    - --conf
    - spark.kubernetes.executor.node.selector.bebida=node
    - --conf
    - spark.kubernetes.container.image=ryaxtech/spark-on-k8s:v0.2.0
    - --conf
    - spark.driver.host=spark-app-pi
    - --conf
    - spark.driver.port=4041
    - --conf
    - spark.driver.bindAddress=0.0.0.0
    # FIXME Need a shared FileSystem for that and user1 NFS has rights issue
    #- --conf
    #- spark.eventLog.enabled=true
    #- --conf
    #- spark.eventLog.dir=/data/results/user1/%EXPE_DIR%
    # App to run
    - --class
    - org.apache.spark.examples.SparkPi
    - file:///lib/spark-3.2.2/examples/jars/spark-examples_2.12-3.2.2.jar
    - "50000"
    command:
    - bash
    - /data/spark-entrypoint.sh
    image: ryaxtech/spark-on-k8s:v0.2.0
    imagePullPolicy: IfNotPresent
    name: spark-driver-1
    volumeMounts:
      - name: config-volume
        mountPath: /etc/config
      - name: result-dir
        mountPath: /data/results
  restartPolicy: Never
  volumes:
    - name: result-dir
      nfs:
        server: server
        path: /users # The exported directory

    - name: config-volume
      configMap:
        # Provide the name of the ConfigMap containing the files you want
        # to add to the container
        name: pod-templates
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pod-templates
data:
  executor.yaml: |
    metadata:
      annotations:
        ryax.tech/bebida: "exclude"
    spec:
      containers:
        - command:
          - bash
          - /data/spark-entrypoint.sh
      tolerations:
      - key: "bebida"
        operator: "Exists"
        effect: "NoSchedule"
      nodeSelector:
        bebida: node
---
apiVersion: v1
kind: Service
metadata:
  name: spark-app-pi
spec:
  clusterIP: None
  selector:
    run: spark-app-pi
  ports:
    - protocol: TCP
      port: 4041
      targetPort: 4041

